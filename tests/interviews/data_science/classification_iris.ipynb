{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "## Load pandas and sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,balanced_accuracy_score,roc_curve,auc,f1_score,precision_recall_fscore_support,confusion_matrix,plot_roc_curve,plot_confusion_matrix\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "\n",
    "from patsy import dmatrices, dmatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Loading the R magic along with the tidyverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n",
      "\n",
      "R[write to console]: ✔ ggplot2 3.3.5     ✔ purrr   0.3.4\n",
      "✔ tibble  3.1.3     ✔ dplyr   1.0.7\n",
      "✔ tidyr   1.1.3     ✔ stringr 1.4.0\n",
      "✔ readr   1.4.0     ✔ forcats 0.5.1\n",
      "\n",
      "R[write to console]: ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "✖ dplyr::filter() masks stats::filter()\n",
      "✖ dplyr::lag()    masks stats::lag()\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <span>StrVector with 17 elements.</span>\n",
       "        <table>\n",
       "        <tbody>\n",
       "          <tr>\n",
       "          \n",
       "            <td>\n",
       "            'forcats'\n",
       "            </td>\n",
       "          \n",
       "            <td>\n",
       "            'stringr'\n",
       "            </td>\n",
       "          \n",
       "            <td>\n",
       "            'dplyr'\n",
       "            </td>\n",
       "          \n",
       "            <td>\n",
       "            ...\n",
       "            </td>\n",
       "          \n",
       "            <td>\n",
       "            'datasets'\n",
       "            </td>\n",
       "          \n",
       "            <td>\n",
       "            'methods'\n",
       "            </td>\n",
       "          \n",
       "            <td>\n",
       "            'base'\n",
       "            </td>\n",
       "          \n",
       "          </tr>\n",
       "        </tbody>\n",
       "        </table>\n",
       "        "
      ],
      "text/plain": [
       "<rpy2.robjects.vectors.StrVector object at 0x7f00f8495240> [RTYPES.STRSXP]\n",
       "R classes: ('character',)\n",
       "['forcats', 'stringr', 'dplyr', 'purrr', ..., 'utils', 'datasets', 'methods', 'base']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext rpy2.ipython\n",
    "%R R.version.string\n",
    "%R library(tidyverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Training and testing sets\n",
    "\n",
    "* Load the data\n",
    "* Define the model matrices\n",
    "* Split into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/iris_tidy.csv')\n",
    "\n",
    "def get_formula(response_col, data):\n",
    "    return ' ~ 0 + ' + ' + '.join(data.columns.difference([response_col]))\n",
    "\n",
    "formula = get_formula('Species', data)\n",
    "x = dmatrix(formula, data)\n",
    "\n",
    "# lb = LabelBinarizer()\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(data['Species'])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('clf', OneVsOneClassifier(estimator=LogisticRegression()))])\n",
      "Accuracy: 1.0\n",
      "Balanced Accuracy: 1.0\n",
      "Precision/Recall/F1 Score: (1.0, 1.0, 1.0)\n",
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('clf', LogisticRegression(multi_class='ovr'))])\n",
      "Accuracy: 0.9666666666666667\n",
      "Balanced Accuracy: 0.9629629629629629\n",
      "Precision/Recall/F1 Score: (0.9694444444444444, 0.9666666666666667, 0.9664109121909632)\n",
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  8  1]\n",
      " [ 0  0 11]]\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('clf', LogisticRegression(multi_class='multinomial'))])\n",
      "Accuracy: 1.0\n",
      "Balanced Accuracy: 1.0\n",
      "Precision/Recall/F1 Score: (1.0, 1.0, 1.0)\n",
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "models = [\n",
    "    OneVsOneClassifier(LogisticRegression()),\n",
    "    LogisticRegression(multi_class='ovr'),\n",
    "    LogisticRegression(multi_class='multinomial')\n",
    "]\n",
    "\n",
    "def pipe(model):\n",
    "  return Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', model)\n",
    "  ])\n",
    "\n",
    "scaled_models = [pipe(model) for model in models]\n",
    "\n",
    "def metrics(y_test, y_pred):\n",
    "    print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "    print('Balanced Accuracy:', balanced_accuracy_score(y_test, y_pred))\n",
    "    # print('F1 Score:', f1_score(y_test, y_pred, average='weighted'))\n",
    "    print('Precision/Recall/F1 Score:', precision_recall_fscore_support(y_test, y_pred, average='weighted')[:-1])\n",
    "    # print('Precision/Recall/F1 per class:')\n",
    "    # print(np.vstack(precision_recall_fscore_support(y_test, y_pred, average=None)[:-1]))\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "for model in scaled_models:\n",
    "  print(model)\n",
    "  model.fit(x_train, y_train)\n",
    "  y_pred = model.predict(x_test)\n",
    "  metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[183.15310137563065, 58.21959125863561, 265.16537796870114, 95.55847291989042]\n"
     ]
    }
   ],
   "source": [
    "# Let's just check the multicolinearity\n",
    "from statsmodels.stats.outliers_influence import \\\n",
    "    variance_inflation_factor as vif\n",
    "\n",
    "print([vif(x_train, i) for i in range(x_train.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "gs.best_score_: 0.9504093976771995\n",
      "gs.best_params_: {'clf__C': 10.0, 'clf__max_iter': 10000, 'clf__multi_class': 'multinomial', 'clf__penalty': 'l1', 'clf__solver': 'saga'}\n",
      "gs.best_estimator_[\"clf\"].coef_: [[-4.06767085 -3.08136947  0.          1.51433645]\n",
      " [ 0.          0.          1.15988466  0.        ]\n",
      " [ 9.5200361   8.20253017  0.         -2.1390414 ]]\n",
      "Accuracy: 1.0\n",
      "Balanced Accuracy: 1.0\n",
      "Precision/Recall/F1 Score: (1.0, 1.0, 1.0)\n",
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "fold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid = {\n",
    "  'clf__C': np.power(10.0, np.arange(-4, 3)),\n",
    "  'clf__multi_class': ['multinomial'],\n",
    "  'clf__solver': ['saga'],\n",
    "  'clf__penalty': ['l1'],\n",
    "  'clf__max_iter': [10000]\n",
    "}\n",
    "\n",
    "clf = pipe(LogisticRegression())\n",
    "# clf.fit(x_train, y_train)\n",
    "gs = GridSearchCV(clf, grid,scoring='f1_weighted',cv=fold, verbose=1)\n",
    "gs.fit(x_train, y_train)\n",
    "print ('gs.best_score_:', gs.best_score_)\n",
    "print ('gs.best_params_:', gs.best_params_)\n",
    "print('gs.best_estimator_[\"clf\"].coef_:', gs.best_estimator_['clf'].coef_)\n",
    "y_pred = gs.predict(x_test)\n",
    "print(metrics(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Balanced Accuracy: 1.0\n",
      "Precision/Recall/F1 Score: (1.0, 1.0, 1.0)\n",
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "None\n",
      "[0.17583333 0.45166667 0.01583333 0.01583333]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(random_state=0)\n",
    "forest.fit(x_train, y_train)\n",
    "y_pred = forest.predict(x_test)\n",
    "print(metrics(y_test, y_pred))\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "result = permutation_importance(\n",
    "    forest, x_train, y_train, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "print(result.importances_mean)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0b7713b2f7d4923c599d0c7563a498e33709e74e47bc6b228ba17d3a2d0d202c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('ml': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
